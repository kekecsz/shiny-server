---
title: "The Transparent Psi Project, ..."
author: "Zoltan Kekecs, Balazs Aczel, Bence Palfi, Barnabas Szaszi, Peter Szecsi, Mark Zrubka, Marton Kovacs, Umberto Picchini, ..."
date: "June 17, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# The Transparent Psi Project
#### author: Zoltan Kekecs, Balazs Aczel, Bence Palfi, Barnabas Szaszi, Peter Szecsi, Mark Zrubka, Marton Kovacs, Umberto Picchini, ...


<mark>This is an Rmarkdown document that is the preprint of our manuscript. By reloading this tab the document is updated based on the latest data.
</mark>



## Introduction

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Recent reports on the lack of reproducibility of important psychological findings and growing evidence for a systematic positive bias in the published research reports are often interpreted as indicators of a ‘confidence crisis’ in psychological science (summary REF). ‘Questionable Research Practices’ (QRPs, Steneck, 2006) and poor protocol delivery have been suggested to lie behind this calamity (REF). Skepticism about the quality and trustworthiness of studies is on the rise in most areas of psychological science, and life sciences in general since the beginning of this confidence crisis. The increased apprehension in the field can lead to the early or unwarranted dismissal of research findings.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This issue is especially tangible in relation to studies testing controversial hypotheses or with findings contradictory to the status quo. In these cases, the notion that the finding was artificially generated by QRPs or poor execution of the protocol may seem more parsimonious than the proposed unconventional theory. However, such suspicions are very hard to test. With the exception of a few rare cases where there is actual evidence of QRPs, it is not possible to prove or disprove as-intended protocol delivery or that a particular study is free of QRPs.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One might argue that replications would negate this issue. That is, the trustworthiness of a finding can simply be tested by repeating the study, and seeing whether the finding would replicate. Unfortunately, the lack of established methods to verify study integrity hampers the credibility of replications just as well as that of original studies. QRPs and protocol deviations are partly outlets for generating experimenter expectancy effects. So the failure to replicate a particular finding could be also explained by bias introduced through QRPs and sloppy study execution. Thus, supporters and skeptics of a particular finding can easily find themselves in a deadlock.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Importantly, replication is a relatively costly method for checking credibility. It requires almost the same amount of resources, or more if we would like to adhere to recent replication standards, than the original study. This is not worth it if the results of the replication can be easily dismissed on ground of potential QRPs and protocol deviations, which cannot be refuted. It may be easier to simply disregard the controversial findings altogether, until they are proven in a ‘really good study’. Another common practice to dismiss failed replications by the supporters of the original findings is to post-hoc label them ‘conceptual replications’ that deviate in some minor detail from the original study that could have caused the discrepant results. More generally, post-hoc criticism of study design can make the whole research, be it an original study or a replication attempt, a waste of resources.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The risk of un-disprovable post-hoc accusations of QRPs or poor protocol delivery and the possibility of post-hoc criticism of study design decrease the relative value of conducting studies, this, together with the fact that we rely on resource-intensive replications to prove credibility, makes the research endeavour costly, and this relative cost increases together with the scepticism in the field. Therefore, there is a critical need for methodological advances that can cost-effectively increase the credibility and acceptability of original research, and we need to set up clear criteria on how to conduct trustworthy research, that can be adhered to in future scientific ventures.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First, the risk of post-hoc criticism could be decreased by peer review of the research plan before the start of the study. With the raise of registered reports, the risk for conducting a study with a fundamentally flawed methodology is somewhat decreased, since the researchers can get expert feedback about study design before initiating data collection. However, the registered reports are still constrained by the limitations of the peer-review system, for example, that only a handful of reviewer get to comment on the proposal, which offers limited protection from post-hoc criticism (especially interest-driven criticism) from others. These limitations could be overcome by expanding the concept of registered report peer review to a community-based peer-review of the research plan.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Second, developing methodologies that decrease or eliminate opportunities for QRPs and which make as-intended protocol delivery verifiable could be the solution to re-instate confidence in original research. Recently, there has been a rise in initiatives for transparency which could help reduce the prevalence of QRPs, such as encouragement of pre-registration of experiments by professional and governmental agencies, a call to publish research reports irrespective of their outcome, making reports available to a wider audience through self-archiving and open access publication, open publishing platforms, data repositories, and initiatives for large scale multi-lab replications. Furthermore, [best practice guidelines][1] have been set up to further improve credibility and integrity of research. However, inconsistencies in protocol delivery, result driven exclusion or imputation of data, and fraud are not prevented by these interventions and guidelines, because the study and data collection themselves are still performed ‘in the dark’. So, to eliminate this last ‘safe haven’ for QRPs and sloppy protocol execution, further innovations are in order.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;An area where tensions regarding trustworthiness are at an extreme is parapsychology. Recently, several publications reported positive results in support of ‘psi’ phenomena (Bem, 2011; Storm, Tressoldi, & Di Risio, 2010). Even though some of these report were published in high profile psychology journals, they generally met poor reception. For example, in his Experiment 1, Bem (2011) found a statistically significant higher than chance prediction rate of future randomly determined events (if correct guesses were reinforced with erotic stimuli). The authors interpreted this result as evidence supporting that humans have future telling (precognition) abilities. The interpretation that the results would be evidence for human extrasensory perception (ESP) was criticized on several accounts (Fiedler & Krueger, 2013; Rouder & Morey, 2011; Schwarzkopf, 2014). Those who offered counter-explanations for the positive findings usually mentioned some type of QRP, and problems with the execution of the studies (Wagenmakers, Wetzels, Borsboom, Kievit, & van der Maas, 2015; Wagenmakers, Wetzels, Borsboom, & van der Maas, 2011).
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The scientific discourse surrounding these parapsychology studies played an important role in initiating the reformist movement in psychological science. So parapsychology experiments in general, and Bem's (2011) experiments in particular provide an excellent opportunity to test the impact and effectiveness of innovations enabling verifiable credibility. Thus, we chose to implement the credibility-enhancing methodologies devised by our group in a large scale replication of Bem’s (2011) Experiment 1, in order to test the effects of these interventions in a setting where they are expected to have a very high impact.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One way to increase confidence in the reliability of findings of ESP studies is to develop a study design jointly between psi proponent and skeptical laboratories (Schlitz, Wiseman, Watt, & Radin, 2006; Wagenmakers et al., 2015). The joint development of a study between proponents and opponents of a theory can be highly beneficial in improving the acceptability of the findings, because foreseeable methodological issues can be worked out before the execution of the study. Parties can also agree on the interpretation of different possible results in advance, thus making the conclusions of the study mutually acceptable. Credibility of research can also be improved this way by both parties making clear what kinds of assurances are required for a study to be considered credible and implementing them in the study design. Such joint studies are very rare at the moment and usually only involve a small number of researchers. However, if done on a large enough scale involving a significant portion of the stakeholders on the field, this approach could promote results and interpretations that are credible and acceptable for both proponents and opponents of the theory.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In Stage 1 of our project we have submitted our research plan to a **‘Consensus Design Process’**, a process involving a large number of stakeholders, all of whom have contributed to the debate surrounding Bem's (2011) experiments. These stakeholders were identified and approached systematically via a systematic review of the literature. The initial study protocol of the replication study was then peer reviewed by these stakeholders, and the study protocol was iteratively amended until a consensus was reached on the acceptability of the design using a ‘reactive-Delphi’ process (McKenna, 1994). The final review panel included 23 researchers with a roughly equal mix of proponents and opponents of the ESP-theory, among them the author of the original study. See details in another publication.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In our current paper we describe the implementation of the methodologies that promote verifiable credibility as applied in the case of a replication of Bem’s (2011) Experiment 1. Specifically, we use:
 - Real-time born open data: as data is being collected, it is
   immediately in real-time being pushed through to a publicly accessible database keeping an audit trail, ensuring data integrity.
 - Consensus Design Process.
 - Consensus-based pre-registration of conclusions for each possible outcome of the study related to the confirmatory hypothesis testing.
 - Verifying as-intended protocol delivery through video recordings of scripted trial sessions.
 - Validation of software codes by an independent party.
 - Involving independent research auditors to verify study integrity.
 - Recording any potential protocol deviations.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With the replication of Bem’s (2011) Experiment 1, our aim is to provide an example for the use of these methods in action. We believe that this in-practice example will help to demonstrate the feasibility and the effectiveness of the credibility-enhancing methodologies better than simply writing about them in and abstract way.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the study we will contrast the likelihood of our observed data under the following two models:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Model 1 (M1) assumes that some humans can guess the future randomly determined position of a target at higher than chance success rate when correct guesses are reinforced by an erotic image. (Note that this is a one-sided model, thus, statistical tests will be equally one-sided.)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Model 0 (M0) assumes that humans cannot guess the future randomly determined position of a target at higher than chance success rate when correct guesses are reinforced by an erotic image.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(There are no other models or hypotheses tested in the confirmatory part of the analysis and no exploratory models can replace these confirmatory models.)


## Methods

..

## Results

```{r packages, include=FALSE, cache=FALSE}


######################################################################
#                                                                    #
#                           Load packages                            #
#                                                                    #
######################################################################

library(HDInterval) # needed to calcluate HDI credible intervals in the Bayesian parameter estimation robustness test
library(ggplot2) # for visualization
library(emdist) # to calcluate earth mover's distance (EMD)
```

```{r calculations, echo = FALSE}
######################################################################
#                                                                    #
#                            Functions                               #
#                                                                    #
######################################################################

######################################################################
#                  Bayes factor calculation functions                #
######################################################################


### Functions for Bayes factor caclulation using beta prior
# These functions are required to run the Bayes factor analysis 
# we thank Richard Morey for his help in developing these functions


fullAlt_beta = Vectorize(function(p, y, N, alpha, beta){
  exp(dbinom(y, N, p, log = TRUE) + dbeta(p, alpha, beta, log = TRUE)) 
},"p")

normalize_beta = function(alpha, beta, interval){
  diff(pbeta(interval, alpha, beta))
}

restrictedAlt_beta = function(p,y,N,y_prior,N_prior,interval){
  alpha = y_prior + 1
  beta = N_prior - y_prior + 1
  fullAlt_beta(p, y, N, alpha, beta) / normalize_beta(alpha, beta, interval) * (p>interval[1] & p<interval[2])
}

margLike_beta = function(y, N, y_prior, N_prior, interval){
  integrate(restrictedAlt_beta, interval[1], interval[2], 
            y = y, N = N, y_prior = y_prior, N_prior = N_prior, interval = interval)[[1]]
}

BF01_beta = Vectorize(function(y, N, y_prior, N_prior, interval, null_prob){
  dbinom(y, N, null_prob) / margLike_beta(y = y, N = N, y_prior = y_prior, N_prior = N_prior, interval = interval)
},"y")


######################################################################
#                       Other supporting functions                   #
######################################################################

### Function calculating the highest density interval using sampling
# We use hdi() from the library(HDInterval)
# this function is needed for the Bayesian parameter estimation robustness test

mode_HDI <- function(scale, density, crit_width = 0.95, n_samples = 1e5){
  samp <- sample(x = scale, size = n_samples, replace = TRUE, prob = density)
  hdi_result = hdi(samp, credMass=crit_width)
  result = c(scale[which(density == max(density))], # mode
             hdi_result[1], # lower bound
             hdi_result[2]) # upper bound
  
  # only needed for the names of the result
  Crit_lb = (1-crit_width)/2
  Crit_ub = crit_width + (1-crit_width)/2
  
  names(result) = c("mode", paste(Crit_lb*100, "%", sep = ""), paste(Crit_ub*100, "%", sep = ""))
  return(result)
}




######################################################################
#                                                                    #
#                          Data parameters                           #
#                                                                    #
######################################################################

### analysis parameters
# these are the analysis parameters currently specified in our protocol

# number of erotic trials performed by participants in the study (if no missing trials)
trial_size_per_participant = 18

# probability of success if M0 is true
M0_prob = 0.5

# interim analysis points (in total number of erotic trials performed)
when_to_check = c(30060, 37836, 45612, 53406, 61182, 68958, 76734, 84528, 92304, 100080)

# thresholds to infer support for M0 (high) or M1 (low)
Inference_threshold_BF_high = 25
Inference_threshold_BF_low = 1/Inference_threshold_BF_high

# this information is used both for calculating replication Bayes factor, and the Bayesian parameter estimation robustness test. 
# Here we use data from Bem's experiment 1, 828 successes within 1560 erotic trials
y_prior = 828 #number of successes in erotic trials in Bem's experiment 1
N_prior = 1560 # number of erotic trials in Bem's experiment 1


# smallest effect size of interest in the NHST equivalence test 
minimum_effect_threshold_NHST = 0.01
# p threshold for the NHST proportion test robustness test
Inference_threshold_robustness_NHST = 0.005

# in the Bayesian parameter estimation robustness test this will determine the region of practical 
#equivalence (ROPE) interval. The ROPE is interpreted similarly to SESOI, but not entireli the same. 
# See Kruschke, J. K., & Liddell, T. M. (2017). The Bayesian New Statistics: Hypothesis testing, 
# estimation, meta-analysis, and power analysis from a Bayesian perspective. 
# Psychonomic Bulletin & Review, 1-29. 
minimum_effect_threshold_Bayes_Par_Est = 0.006
# this threshold is used to set the HDI width to check against the ROPE in the Bayesian parameter 
# estimation robustness test, if ths parameter is set to 0.05 for example, it means that we would 
# expect that 95% of the probability mass would be within the ROPE to accept a hypothesis
Inference_threshold_robustness_Bayes_Par_Est = 0.05 

    ######################################################################
    #                                                                    #
    #                         Data management                            #
    #                                                                    #
    ######################################################################
    
    # get up to date date from github
    pilot_data_pre <- read.csv("https://raw.githubusercontent.com/gy0p4k/transparent-psi-results/master/results.csv")
    
    # only read pilot sessions
    # pilot sessions are marked with session_type = "pilot" and they were all collected in the lab with the laboratory_ID_code = "lab_ELTE_01"
    lab_ID <- "lab_ELTE_01"
    pilot_data <- pilot_data_pre[pilot_data_pre[,"session_type"] == "pilot" & pilot_data_pre[,"laboratory_ID_code"] == "lab_ELTE_01", ]
    
    
    # Number of participants tested in the pilot test
    sample_size_participants = length(unique(pilot_data[, "participant_ID"]))
    
    
    pilot_data[, "trial_number"] = as.numeric(pilot_data[, "trial_number"])
    
    #extract data from erotic trials 
    data_BF = pilot_data[!is.na(pilot_data[, "trial_number"]) & pilot_data[, "reward_type"] == "erotic", ]
    
    data_BF[,"participant_ID"] = droplevels(data_BF[,"participant_ID"])
    
    
    ######################################################################
    #                                                                    #
    #                          Data analysis                             #
    #                                                                    #
    ######################################################################
    
    
    ######################################################################
    #                       Primary confirmatory test                    #
    ######################################################################
    
    #================================================================#
    #                Calculate number of successes                   #
    #================================================================#
    
    # number of successes and total N of trials
    sides_match = data_BF[,"guessed_side"] == data_BF[,"target_side"]
    successes = sum(sides_match)
    total_N = nrow(data_BF)
    
    
    #================================================================#
    #        Calculating Bayes factors using different priors        #
    #================================================================#
    
    ### Replication Bayes factor, with the Bem 2011 experiment 1 results providing the prior information
    
    BF_replication <- BF01_beta(y = successes, N = total_N, y_prior = y_prior, N_prior = N_prior, interval = c(0.5,1), null_prob = M0_prob) #numbers higher than 1 support the null
    
    
    ### Bayes factor with uniform prior
    # using a non-informative flat prior distribution with alpha = 1 and beta = 1
    
    BF_uniform <- BF01_beta(y = successes, N = total_N, y_prior = 0, N_prior = 0, interval = c(0.5,1), null_prob = M0_prob) #numbers higher than 1 support the null
    
    
    ### Bayes factor with BUJ prior
    # the BUJ prior is calculated from Bem's paper where the prior distribution is defined as a
    # normal distribution with a mean at 0 and 90th percentele is at medium effect size d = 0.5 
    # (we asume that this is one-tailed). Source: Bem, D. J., Utts, J., & Johnson, W. O. (2011). 
    # Must psychologists change the way they analyze their data? Journal of Personality and Social Psychology, 101(4), 716-719.
    # We simulate this in this binomial framework with a one-tailed beta distribution with alpha = 7 and beta = 7.
    # This distribution has 90% of its probability mass under p = 0.712, which we determined 
    # to be equivalent to d = 0.5 medium effect size. We used the formula to convert d to log odds ratio logodds = d*pi/sqrt(3), 
    # found here: Borenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). 
    # Converting Among Effect Sizes. In Introduction to Meta-Analysis (pp. 45-49): John Wiley & Sons, Ltd.
    # Then, log odds ratio vas converted to probability using the formula: p = exp(x)/(1+exp(x))
    # The final equation: exp(d*pi/sqrt(3))/(1+exp(d*pi/sqrt(3)))
    
    BF_BUJ <- BF01_beta(y = successes, N = total_N, y_prior = 6, N_prior = 12, interval = c(0.5,1), null_prob = M0_prob) #numbers higher than 1 support the null
    
    
    #================================================================#
    #                    Main analysis inference                     #
    #================================================================#
    
    # determine inference (supported model) based on the Bayes factors calculated above  
    if(Inference_threshold_BF_low >= max(c(BF_replication, BF_uniform, BF_BUJ))) {
      inference_BF = "M1"
      break} else if(Inference_threshold_BF_high <= min(c(BF_replication, BF_uniform, BF_BUJ))) {
        inference_BF = "M0"
        break} else {inference_BF = "Inconclusive"}
    
    
    ##################################################
    #                 Robustness tests               #
    ##################################################
    
    #================================================================#
    #               Robustness test of BF results with NHST          #
    #================================================================#
    
    # robustness of BF results is tested with NHST proportion tests
    # here we perform both an equality test and an equivalence test to draw statistical inference
    
    
    # equality test
    equality_test_p = prop.test(x = successes, n = total_N, p = M0_prob, alternative = "greater")$p.value
    
    # equivalence test
    equivalence_test_p = prop.test(x = successes, n = total_N,
                                   p = M0_prob+minimum_effect_threshold_NHST, 
                                   alternative = "less")$p.value
    
    # descritives of the frequentist estimate
    pbar = successes/total_N
    SE = sqrt(pbar * (1-pbar)/total_N)
    E = qnorm(.9975) * SE
    proportion_995CI = round(pbar + c(-E , E), 3)
    
    
    
    # making inference decision
    if((Inference_threshold_robustness_NHST > equality_test_p) & 
       (Inference_threshold_robustness_NHST <= equivalence_test_p)){
      inference_robustness_NHST = "M1"} else if((Inference_threshold_robustness_NHST > equivalence_test_p) & 
                                                (Inference_threshold_robustness_NHST <= equality_test_p)){
        inference_robustness_NHST = "M0" 
      } else {inference_robustness_NHST = "Inconclusive"}
    
    
    #=======================================================================#
    #   Robustness test of BF results with Bayesian parameter estimation    #
    #=======================================================================#
    
    # robustness of BF results is tested by calculating HDI of the posteriro distribution and checking its relation to
    # the region of practical equivalence (ROPE), promoted in Kruschke, J. K., & Liddell, T. M. 
    # (2017). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power 
    # analysis from a Bayesian perspective. Psychonomic Bulletin & Review, 1-29. 
    
    # calculate posterior distribution using beta distribution
    prior_alpha = y_prior + 1
    prior_beta = N_prior-y_prior+1
    
    posterior_alpha = prior_alpha + successes
    posterior_beta = prior_beta + total_N - successes
    
    scale = seq(0, 1, length = 1001)
    posterior_density = dbeta(scale, posterior_alpha, posterior_beta)
    
    # calculate HDI for the posterior distribution
    # (here we calculate the upper and lower bound of the 90% of the probability mass
    # because we use a one-tailed test. This means that the total probability mass below
    # the upper bound of the 90% HDI will be 95%)
    hdi_result = mode_HDI(scale = scale, density = posterior_density, crit_width = 1-Inference_threshold_robustness_Bayes_Par_Est*2, n_samples = 1e6)
    
    # parameters for decision making
    HDI_lb = hdi_result[2]
    HDI_ub = hdi_result[3]
    
    # making inference decision
    ROPE = M0_prob+minimum_effect_threshold_Bayes_Par_Est
    if(HDI_lb >= ROPE){inference_robustness_Bayes_Par_Est = "M1"
    } else if(HDI_ub <= ROPE){inference_robustness_Bayes_Par_Est = "M0"
    } else {inference_robustness_Bayes_Par_Est = "Inconclusive"}
    
    
    #=======================================================================#
    #      Determine final inference of all robustness tests combined       #
    #=======================================================================#
    
    # the main analysis inference is only robust if all robustness tests came to the same inference as the main test
    inferences = c(inference_robustness_NHST, inference_robustness_Bayes_Par_Est)
    inference_robustness = if(all(inferences == inferences[1])){inferences[1]} else {"mixed"}
    
    Robust = if(inference_BF == inference_robustness){"robust"} else {"not robust"}
    
    
    ######################################################################
    #                                                                    #
    #                   Interpretation and visualization                 #
    #                                                                    #
    ######################################################################
    
    
    # visualize results
    # ALL THREE BF values need to cross the threshold for the interpretation to be accepted
    
    BF_results_for_plotting = cbind(as.data.frame(c(BF_replication, BF_uniform, BF_BUJ)), c("BF_replication", "BF_uniform", "BF_BUJ"))
    names(BF_results_for_plotting) = c("Bayes_factor_01", "BF_type")
    
    plot <- ggplot(BF_results_for_plotting, aes(y = Bayes_factor_01, x = BF_type))+
      geom_point()+
      geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=c(Inference_threshold_BF_high), ymax=c(Inf)), alpha = 0.2, fill=c("pink"))+
      geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=c(Inference_threshold_BF_low), ymax=c(Inference_threshold_BF_high)), alpha = 0.2, fill=c("grey80"))+
      geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=c(0), ymax=c(Inference_threshold_BF_low)), alpha = 0.2, fill=c("lightgreen"))+
      geom_point(size = 3.5, shape = 21, fill = "white")+
      scale_y_log10(limits = c(0.005,200), breaks=c(0.01, Inference_threshold_BF_low, 0.1, 0.33, 0, 3, 10, Inference_threshold_BF_high, 100))+
      geom_hline(yintercept = c(Inference_threshold_BF_low, Inference_threshold_BF_high), linetype = "dashed")+
      geom_text(aes(x=0.5, y=c(100, 1, 0.01), label=c("Supports M0", "Inconclusive", "Supports M1"), angle = 270))
    
    
    
    
    ##################################################
    #       Report of confirmatory analysis          #
    ##################################################
    
    # number of erotic trials performed (without the last participant, whose data might
    # be cut in half because of the interim stopping point)
    numtrials_without_last_part = nrow(data_BF[data_BF[,"participant_ID"] != data_BF[nrow(data_BF),"participant_ID"],])
    # number of missing trials/data points (minus the last participant, who might be cut in half)
    missing_data_points = (length(unique(data_BF[data_BF[,"participant_ID"] != data_BF[nrow(data_BF),"participant_ID"], "participant_ID"])) * trial_size_per_participant) - numtrials_without_last_part 
    
    
    general_text = paste("As of  ", Sys.time(), "  (time zone:  ", Sys.timezone(), "), a total of ",  total_N, " erotic trials were observed, gathered from a total of ",
                         sample_size_participants, " participants.", " There has been ", missing_data_points, " (", round(missing_data_points/total_N*100,2),"%) missing data points due to unfinished sessions.", 
                         " We observed a total of ", round(mean(sides_match), 4)*100, "% successful guesses within ",
                         total_N, " erotic trials (99.5% CI = ", proportion_995CI[1]*100, "%", ", ", proportion_995CI[2]*100, "%", 
                         "; posterior mode = ", hdi_result[1]*100, "%", ", posterior 90% HDI = ", HDI_lb*100, "%", ", ",
                         HDI_ub*100, "%", ").", sep = "")
    
    robustness_text = if(Robust == "robust"){paste(" The results proved to be robust to different statistical approaches, increasing our confidence in our inference.")} else if(Robust == "not robust"){
      paste(" However, the results did not prove to be robust to different statistical approaches.")}
    
final_text = if(inference_BF == "M1"){ 
      paste(general_text, " Observing this success rate is ", round(1/max(c(BF_replication, BF_uniform, BF_BUJ)),0),
            " times more likely if humans can guess future randomly determined events than if they are guessing randomly. Taken at face value, the data provide strong evidence that the probability of successfully guessing later computer-generated random events is higher than chance level as previously reported by Bem (2011) and others (Bem, Tressoldi, Rabeyron, & Duggan, 2015).",
            robustness_text, sep = "")
    } else if(inference_BF == "M0"){
      paste(general_text, " Observing this success rate is ", round(min(c(BF_replication, BF_uniform, BF_BUJ)),0),
            " times more likely if humans are guessing randomly than if they can guess future randomly determined events. Taken at face value, the data provide strong evidence that the probability of successfully guessing later computer-generated random events is not higher than chance level as previously reported by Bem (2011) and others (Bem, Tressoldi, Rabeyron, & Duggan, 2015).",
            robustness_text, sep = "")
    } else if(inference_BF == "Inconclusive" & max(c(BF_replication, BF_uniform, BF_BUJ)) < 1){
      paste(general_text, " Observing this success rate is ", round(1/max(c(BF_replication, BF_uniform, BF_BUJ)),0),
            " times more likely if humans can guess future randomly determined events than if they are guessing randomly. However, this study outcome did not reach the pre-specified criteria of strong support for either model.",
            robustness_text, sep = "")
    } else if(inference_BF == "Inconclusive" & min(c(BF_replication, BF_uniform, BF_BUJ)) > 1){
      paste(general_text, " Observing this success rate is ", round(min(c(BF_replication, BF_uniform, BF_BUJ)),0),
            " times more likely if humans are guessing randomly than if they can guess future randomly determined events. However, this study outcome did not reach the pre-specified criteria of strong support for either model.",
            robustness_text, sep = "")
    } else {paste(general_text, "However, this study outcome did not reach the pre-specified criteria of strong support for either model.",
                  robustness_text, sep = "")}



    ######################################################################
    #                                                                    #
    #                         Exploratory analysis                       #
    #                                                                    #
    ######################################################################
    # The existence of individual differences between participants will be evaluated 
    # using a frequentist approach where we assume that it is possible that only a small 
    # subgroup of the population we sample from is capable of predicting future events
    # with better than chance accuracy.
    
    # EXPLORATORY ANALYSIS RESULTS WILL NOT AFFECT THE CONCLUSIONS OF OUR STUDY
    
    #=======================================================================#
    #           Comparison of expected and observed distributions           #
    #=======================================================================#
    
    # calculate proportion of successful guesses for each participant in the observed data
    data_BF_split = split(data_BF, f = data_BF[,"participant_ID"])
    success_proportions_empirical = sapply(data_BF_split, function(x) mean(x[,"guessed_side"] == x[,"target_side"]))
    
    
    # samples 1,000,000 participants from a population with H0 success rate
    # this is used for the stochastic dominance test as the null model
    # we call this the theoretical sample, because it approximates the theoretical null model
    sim_null_participant_num = 1000000
    success_proportions_theoretical <- rbinom(sim_null_participant_num, size = trial_size_per_participant, prob=M0_prob)/trial_size_per_participant
    
    # determine possible values of success rates
    possible_success_rates = 0
    for(i in 1:trial_size_per_participant){
      possible_success_rates[i+1] = round(1/(trial_size_per_participant/i), 2)
    }
    possible_success_rates_char = as.character(possible_success_rates)
    success_proportions_theoretical_char_rounded = as.character(round(success_proportions_theoretical, 2))
    
    
    # Round the success rates of each participant to the nearest "possible success rate" value. 
    # This is necessary to allow for missing trials for some participants, which may generate
    # success rates that are different from the "possible success rate" for those completing all trials. 
    success_proportions_empirical_rounded <- sapply(success_proportions_empirical, function(x) possible_success_rates[which.min(abs(possible_success_rates - x))])
    
    success_proportions_empirical_char_rounded = as.character(round(success_proportions_empirical_rounded, 2))
    
    success_rates_theoretical = NA
    for(i in 1:length(possible_success_rates)){
      success_rates_theoretical[i] = sum(success_proportions_theoretical_char_rounded == possible_success_rates_char[i])
    }
    success_rates_theoretical_prop = matrix(success_rates_theoretical/sum(success_rates_theoretical))
    
    
    success_rates_empirical = NA
    for(i in 1:length(possible_success_rates)){
      success_rates_empirical[i] = sum(success_proportions_empirical_char_rounded == possible_success_rates_char[i])
    }
    success_rates_empirical_prop = matrix(success_rates_empirical/sum(success_rates_empirical))
    
    
    
    # plot 
    histogram_plot_data = as.data.frame(c(success_rates_theoretical_prop, success_rates_empirical_prop))
    histogram_plot_data = cbind(histogram_plot_data, factor(c(rep("Expected if M0 is true", length(success_rates_theoretical_prop)), rep("Observed", length(success_rates_empirical_prop)))))
    histogram_plot_data = cbind(histogram_plot_data, factor(rep(possible_success_rates_char, 2)))
    names(histogram_plot_data) = c("proportion", "group", "success")
    
    figure_1 =  ggplot(histogram_plot_data, aes(y = proportion, x = success, group = group))+
      geom_bar(aes(fill = group), alpha = 0.5, stat = "identity", position = "identity")+
      scale_fill_manual(values = c("darkgrey", "black")) +
      xlab("Successful guess rate") +
      ylab("Proportion") +
      theme(panel.border = element_blank(),
            panel.background = element_blank(),
            panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(),
            legend.position = "bottom",
            axis.line = element_line(colour = "black", size = 1.2),
            axis.text.x = element_text(angle = 90, color = "black", face = "bold", size = 12, margin = margin(1,0,0,0,"mm"), vjust = 0.5),
            axis.text.y = element_text(face = "bold", color = "black", size = 12),
            axis.title = element_text(size = 16))
    
    
    # earth mover's distance
    emd = emd2d(success_rates_theoretical_prop,success_rates_empirical_prop)
 

```






&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`r final_text`


### Main confirmatory analysis results

```{r pressure, echo=FALSE}
suppressWarnings(print(plot))
```

To support any model, all three Bayes Factor values need to pass the threshold

### Robustness analysis

```{r robust_analysis, echo=FALSE}

    plot(scale, posterior_density, type="l", lty=1, xlab="x value", xlim = c(0.45, 0.55),
         ylab="Density")
    abline(v=c(M0_prob, ROPE), lty = c(1, 2))
    
    density_table = as.data.frame(cbind(scale, posterior_density))
    
    height_lim_lb = density_table[density_table[, "scale"] == HDI_lb, "posterior_density"]
    height_lim_ub = density_table[density_table[, "scale"] == HDI_ub, "posterior_density"]
    
    clip(0,1,-10,height_lim_lb)
    abline(v=HDI_lb, lty = 3)
    clip(0,1,-10,height_lim_ub)
    abline(v=HDI_ub, lty = 3)
```

This is the result of the Bayesian parameter estimation robustness analysis using Bayesian updating of prior information. Note that this parameter estimate uses data gathered in the original study of Bem (2011) as prior distribution.

### Exploratory analysis

```{r exploratory_analysis, echo=FALSE}

figure_1

```

Histogram overlay of the expected and observed distribution of successful guess rate

## Discussion

..
